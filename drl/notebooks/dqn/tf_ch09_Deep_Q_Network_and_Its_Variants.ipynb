{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch09_Deep_Q_Network_and_Its_Variants.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMQf+gT7ZWVvbDyDoGo4G+E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psygrammer/fast_and_slow/blob/master/drl/notebooks/dqn/tf_ch09_Deep_Q_Network_and_Its_Variants.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xX90H-jxnBW"
      },
      "source": [
        "# 9. Deep Q Network and Its Variants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeRH8DQY4ZkH"
      },
      "source": [
        "* Fast And Slow / Deep RL - tf2 [1]\n",
        "* 김무성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUtPV6FH4lac"
      },
      "source": [
        "#### 실습 repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suZEO7Cm4ifc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12dfee0d-56ec-4458-8b34-764ef832cdef"
      },
      "source": [
        "!git clone https://github.com/psygrammer/fast_and_slow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fast_and_slow'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 77 (delta 20), reused 43 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (77/77), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3iJQ5iv42c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c15f104-a2ff-4423-da71-807b12c0846e"
      },
      "source": [
        "ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mfast_and_slow\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q7Po6ms43gR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09bdcc7d-3042-41d6-a746-5ebb4840ac70"
      },
      "source": [
        "cd /content/fast_and_slow/drl/notebooks/dqn"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fast_and_slow/drl/notebooks/dqn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqq5QUnP45LZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff0cb37d-5422-4fdc-816d-e2135167c6fa"
      },
      "source": [
        "ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf_ch09_Deep_Q_Network_and_Its_Variants.ipynb\n",
            "tf_ch09_Deep_Q_Network_and_Its_Variants_sol.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nto5nZDK5F3T"
      },
      "source": [
        "#### Install dependancies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75bV0Qza5DE-"
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install pyglet==1.3.2 > /dev/null 2>&1\n",
        "!apt-get install -y xvfb x11-utils python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrEgk8315m2m"
      },
      "source": [
        "#### Imports and Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHa4_Xkw6xah"
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvp5FfH05xFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31fb0a56-6816-43fd-a824-6578006984ea"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7faa4a56d290>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0Eg1IX55xr7"
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAMblZ-i6F6J"
      },
      "source": [
        "-------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRfOJd2o6KUd"
      },
      "source": [
        "# Playing Atari games using DQN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWOJKiQIxd60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de155c29-1b3d-4b50-e18b-dbd38cf6c9e6"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-y6lzFU17PX"
      },
      "source": [
        "# Let's implement the DQN to play the Ms Pacman game. \n",
        "# First, let's import the necessary libraries:\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQehiT0A511m"
      },
      "source": [
        "# Now, let's create the Ms Pacman game environment using Gym:\n",
        "env = gym.make(\"MsPacman-v0\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zrjcgYuGWyj",
        "outputId": "756e30a7-bce2-4a9e-ea08-9b5e722c97c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf_ch09_Deep_Q_Network_and_Its_Variants.ipynb\n",
            "tf_ch09_Deep_Q_Network_and_Its_Variants_sol.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ8guDkS6c7F"
      },
      "source": [
        "env = wrap_env(env) # monitoring"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIhT4HcqGZ4L",
        "outputId": "839c805f-47b9-4796-9d00-8af2b54c5df2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf_ch09_Deep_Q_Network_and_Its_Variants.ipynb      \u001b[0m\u001b[01;34mvideo\u001b[0m/\n",
            "tf_ch09_Deep_Q_Network_and_Its_Variants_sol.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfh37cb-GcS5"
      },
      "source": [
        "ls video"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NwUSlI265LQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e11c3d07-ba7a-4313-d6eb-f3c05b8aa592"
      },
      "source": [
        "env.observation_space"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygdSJHpy6mLx"
      },
      "source": [
        "# Set the state size:\n",
        "state_size = (88, 80, 1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPTc97r662cx"
      },
      "source": [
        "# Get the number of actions:\n",
        "action_size = env.action_space.n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGVoUKg67XZo"
      },
      "source": [
        "## Preprocess the game screen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DSZAzbd72y2"
      },
      "source": [
        "Now, let's define a function called preprocess_state which takes the game state (image of the game screen) as an input and returns the preprocessed game state (image of the game screen):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr6uAbeF636R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a6d5e6-cc96-4e52-8be7-49080f269748"
      },
      "source": [
        "color = np.array([210, 164, 74]).mean()\n",
        "color"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149.33333333333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvzWPB507rxF"
      },
      "source": [
        "def preprocess_state(state):\n",
        "\n",
        "    #crop and resize the image\n",
        "    \n",
        "    #convert the image to greyscale\n",
        "    \n",
        "    #improve image contrast\n",
        "    \n",
        "    #normalize the image\n",
        "    \n",
        "    #reshape the image\n",
        "    \n",
        "    return image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1aKmwd-GqHv"
      },
      "source": [
        "## Defining the DQN class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8AX2mwx74_u"
      },
      "source": [
        "class DQN:\n",
        "    # -- Defining the init method\n",
        "    def __init__(self, state_size, action_size):\n",
        "        pass\n",
        "        #define the state size\n",
        "        \n",
        "        #define the action size\n",
        "        \n",
        "        #define the replay buffer\n",
        "        \n",
        "        #define the discount factor\n",
        "        \n",
        "        #define the epsilon value\n",
        "        \n",
        "        #define the update rate at which we want to update the target network\n",
        "        \n",
        "        #define the main network\n",
        "        \n",
        "        #define the target network\n",
        "        \n",
        "        #copy the weights of the main network to the target network\n",
        "        \n",
        "\n",
        "    # -- Building the DQN\n",
        "    #Let's define a function called build_network which is essentially our DQN. \n",
        "\n",
        "    def build_network(self):\n",
        "        # Define the first convolutional layer:\n",
        "        \n",
        "        # Define the second convolutional layer:\n",
        "        \n",
        "        # Define the third convolutional layer:\n",
        "        \n",
        "        #Flatten the feature maps obtained as a result of the third convolutional layer:\n",
        "        \n",
        "        # Feed the flattened maps to the fully connected layer:\n",
        "        \n",
        "        # Compile the model with loss as MSE:\n",
        "        \n",
        "        # Return the model:\n",
        "        return model\n",
        "\n",
        "\n",
        "    # -- Storing the transition\n",
        "    #We learned that we train DQN by randomly sampling a minibatch of transitions from the\n",
        "    #replay buffer. So, we define a function called store_transition which stores the transition information\n",
        "    #into the replay buffer\n",
        "\n",
        "    def store_transistion(self, state, action, reward, next_state, done):\n",
        "        pass\n",
        "\n",
        "    # -- Defining the epsilon-greedy policy\n",
        "    #We learned that in DQN, to take care of exploration-exploitation trade off, we select action\n",
        "    #using the epsilon-greedy policy. So, now we define the function called epsilon_greedy\n",
        "    #for selecting action using the epsilon-greedy policy.\n",
        "    \n",
        "    def epsilon_greedy(self, state):\n",
        "        \n",
        "        return np.argmax(Q_values[0])\n",
        "\n",
        "\n",
        "    # -- Define the training\n",
        "    #train the network\n",
        "    def train(self, batch_size):\n",
        "        \n",
        "        #sample a mini batch of transition from the replay buffer\n",
        "        \n",
        "        #compute the Q value using the target network\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "                \n",
        "            #compute the Q value using the main network \n",
        "            \n",
        "            #train the main network\n",
        "            \n",
        "    #update the target network weights by copying from the main network\n",
        "    def update_target_network(self):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PewQt36qHELW"
      },
      "source": [
        "## Training the DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y17uh1lRHG8J",
        "outputId": "ae88e1ae-a927-40ab-81f0-1e9947a1fdbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Now, let's train the network. \n",
        "# First, let's set the number of episodes we want to train the network:\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7RbIAqlHIsA",
        "outputId": "42d955ad-0d99-409c-9ac1-e55048069aa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Define the number of time steps\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHLiD0SNHWat",
        "outputId": "5a6517aa-e58a-45b1-ea73-70a76ad6a1ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Define the batch size:\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCBF0o1kHaoS",
        "outputId": "de741b3b-92da-4356-a3ee-46fd1fa1899e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Set the number of past game screens we want to consider:\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQjV8kN5HeQn"
      },
      "source": [
        "# Instantiate the DQN class\n",
        "dqn = DQN(state_size, action_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JadbLC_HnN8"
      },
      "source": [
        "done = False\n",
        "time_step = 0\n",
        "\n",
        "#for each episode\n",
        "for i in range(num_episodes):\n",
        "    #set return to 0\n",
        "    Return = 0\n",
        "    \n",
        "    #preprocess the game screen\n",
        "    \n",
        "    #for each step in the episode\n",
        "    for t in range(num_timesteps):\n",
        "        \n",
        "        #render the environment\n",
        "        env.render()\n",
        "        \n",
        "        #update the time step\n",
        "        \n",
        "        #update the target network\n",
        "        \n",
        "        #select the action\n",
        "        \n",
        "        #perform the selected action\n",
        "        \n",
        "        #preprocess the next state\n",
        "        \n",
        "        #store the transition information\n",
        "        \n",
        "        #update current state to next state\n",
        "        \n",
        "        #update the return\n",
        "        \n",
        "        #if the episode is done then print the return\n",
        "            \n",
        "        #if the number of transistions in the replay buffer is greater than batch size\n",
        "        #then train the network\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqkut4ySHzzl"
      },
      "source": [
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCElcja-H3jr"
      },
      "source": [
        "%ls video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4sQ0rW8H5K3"
      },
      "source": [
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hM9tBajIo5p"
      },
      "source": [
        "--------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7QiW2DQIqc2"
      },
      "source": [
        "# 참고문헌\n",
        "* [1] Deep Reinforcement Learning with Python (2판) - https://www.amazon.com/Deep-Reinforcement-Learning-Python-distributional/dp/1839210680/\n"
      ]
    }
  ]
}